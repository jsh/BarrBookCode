Building a Scalable Architecture with Amazon SQS

Chapter 6: Building a Scalable Architecture with Amazon SQS

 

SQS is the Simple Queueing Service.

 

Basic SQS

 

Try the code in this order:

 

list_queues.php                        # no queues

create_queues.php q1 q2 q3        # three queues

list_queues.php                        # should see them

 

(There was also a list_all_queues.php script, but it was just a superfluous,
older version of list_queues.php, so I removed it to cut down on confusion.)

 

All the queue names are URLs, though your browser can't actually get to them.
(Try it if you want to see for yourself.)

 

The names are a bit long, so set a variable name to one.

 

Q=$(list_queues.php | sed 1q)   # capture the name of one queue

 

Now use it.

 

post_queue.php $Q Message{1..20}

pull_queue.php $Q

 

The messages may not be pulled in the order you posted them, and pull_queue.php
is an infinite loop.

 

The bad news is that this won't work:

 

pull_queue.php $Q &> pull.OUT &

post_queue.php $Q m{1..10}

 

The pull blocks and the process stops. If you know why this shouldn't work (or
doesn't, even if it should), expand this paragraph.

 

The book encourages you to use a pair of terminals, launching the
pull_queue.php in one, and posting from the other.

 

The Big Appl

 

The rest of the chapter develops a multi-server application, in which five
servers are coupled through four SQS queues.

 

A useful analogy is to a pipeline of five processes connected, in a pipeline,
with four pipes.

Jobs traverse the servers, in order, the way they would in a Linux pipeline,
but queues play the role of the pipes.

 

The servers use S3 for interchange of large objects, in the same way you might
use files for IPC between processes in a shell script.

 

You will find yourself using S3 objects more often than you would files because
queue messages have size limitations, while pipes don’t.  Another way to say
this is, “Unix/Linux pipes are an amazing design that we all take for granted.”

 

There's one general-purpose utility, crawl_queue_status.php, which you can use
to monitor the current state of the queues.

 

Here's the flow:

 

load_crawl_urls.php

|

   URL_QUEUE

|

 fetch_page.php

|

   PARSE_QUEUE

|

 parse_page.php

|

   IMAGE_QUEUE

|

 fetch_images.php

|

   RENDER_QUEUE

|

 render_images.php

 

I colored the queues blue because it rhymes. “Blue? Queue!”  I colored the
servers orange for the same reason.

Of these, fetch_images.php, fetch_page.php, and render_images.php require an
S3-bucket name for an argument. Use one of your existing buckets, or make a new
one with the utilities from Chapter 4.

If you run the steps one-at-a-time, you can run crawl_queue_status.php between
steps to see the progress through the pipeline.

When you’re done, go to the S3 bucket, and you’ll see images and a page that’s
the final result. S3Fox will let you look at that page directly, without a
download step!

Edit this page (if you have permission)–Published by Google Docs–Report Abuse
–Updated automatically every 5 minutes
