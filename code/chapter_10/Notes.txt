Advanced AWS

Chapter 10: Advanced AWS

 

“Advanced AWS” means a hodge-podge of unrelated, interesting AWS topics.  As
with other chapters, the code now works for me. If it doesn’t for you,
contribute fixes.

Accounting and Tracking

The shell script accounting-and-tracking invokes the PHP scripts from the first
section of this chapter. Read the section and the script to see what's going
on.

You'll have to download your own usage statistics as csv files to run most of
these.  Do that here.

The example assumes you'll download your own EC2 and S3 usage data and call
them ec2_usage.csv and s3_usage.csv. The book suggests choosing All Usage
Types, with Hours as the granularity.

I've modified the queries so they're not tied to dates from when the text was
being written.

bucket_usage.phpbucket_usage_page.php, like the _page scripts in Chapter 4, is
intended for the server's html/ directory, to be run by the web server. The
basic-examples script cheats, invoking the PHP script from the command-line and
then invoking a browser to look at the output.  Without being run by the web
server, the page isn't pretty, but the concept is clear enough.

(If you make a nicer script to have this display properly, please contribute
it.)

It displays a particular set of usage data for the last week.  When I ran it,
hadn't done enough in that time period to be interesting, so I parameterized
the number of days. If you, too, want to modify the number of days to search
through, look for $ndays in the script.

delete_usage.php

I didn’t see a reference to this script in the chapter, but it came with the
publisher’s code collection, so I made it work anyway. If you find it in the
book, remove this paragraph.

Elastic Block Storage

 

The shell script ebs-cli-examples exercises all the CLI commands in the EBS
section in this chapter. It’s really only there to give you a script that can
run, to compare with should your efforts fail. I built it up for myself as I
tried the book’s commands, to record what did and didn’t work, and to have a
written record of how to invoke them all correctly.

 

 

It does not, however, mount any of the volumes it creates and attaches. The
examples in the book mount the volumes and make changes to them on the target
instance.

 

These commands typically make requests and then return, but the requests then
take time to complete.  The typical paradigm I use, in pseudo-code, is

 

ec2-do-something-to-X

while true; do

 ec2-describe-X | tee /dev/stderr |

   grep -q <still-in-process-indicator> || break

 sleep 30

done

I use tee to let me watch the progress on stderr, while grep’s watching stdout
to figure out when it’s done.

 

Functions in the file watchers.sh encapsulate this paradigm.

 

At one point, I made a utility, launch-tiny-instance, to record the details of
the correct calling convention for ec2-run-instance. Turns out it’s also handy.

Public Data Sets

 

Data? We got yer data right here ….

 

Amazon hosts interesting snapshots of big, public data sets for free, so people
donate them. They're stored as snapshots. You can use them.

 

Here's where to find them.  Some pretty cool collections, I think.

 

If you have an interesting collection to donate, please do so.

 

The previous section in this chapter showed off making snapshots of your own
data, then making volumes of those snapshots, attaching and mounting them. This
section of the chapter demos doing that with someone else's data.

 

The Barr book shows how to create and mount a disk containing seven months of
Wikipedia traffic statistics. The script public-data-sets contains all the
steps, including capturing the DNSname of the instance you created and using
that to ssh to the box.

 

AWS charges you for storage and data transfers, so no, it's not free. Still,
you don't have to generate and upload the data by hand. The Wikipedia traffic
data take up 320G. How long would it take you to grab that and upload it, if
you could find it?

 

After you're logged in, take a look at what you have.

 

sudo mkdir /mnt/sdh

sudo mount /dev/sdh /mnt/sdh

ls /mnt/sdh

ls /mnt/sdh/wikistats

ls /mnt/sdh/wikistats/pagecounts | less

zcat /mnt/sdh/wikistats/pagecounts/pagecounts-20090111-190000.gz | less

sudo umount /mnt/sdh

 

As soon as you've run the public-data-sets script, you start paying for the
space, so if you run it just for fun, detach and delete the volume after you
finish verifying the script has succeeded. I know I did.

 

EBS RAID

 

You can RAID together AWS volumes. You can do object-0riented programming in
the shell. You can bicycle to Alaska.

 

But why would anyone do that?

 

There are lots of good reasons to RAID together AWS volumes.

 

First, because AWS already handles the redundancy part under the covers, the
only flavor that makes sense is RAID 0, which just stripes data across multiple
volumes to provide better performance. You probably won’t need this, but it
does give you a way to finally remember one of the dozens of different RAID
types: zero.

 

Second, you can only allocate EBS volumes of up to a terabyte. Whenever you
need a petabyte of storage on a single disk, you can just create a thousand ,
1TB volumes and RAID(0) them together.

 

Third, if you’re studying for an RHCE exam and want to practice creating
different kinds of RAID arrays, you can instantiate a CentOS AMI, attach some
volumes, and RAID them together.

 

Fourth, … oh, but why should I belabor the obvious? I’m sure you have many good
reasons of your own and can insert them here.

 

The Barr book gives clear instructions for creating a RAID array in the cloud,
which consist of bringing up an instance, creating and attaching some volumes,
logging into the box, and then using standard, Linux, software-RAID commands in
standard, Linux, software-RAID ways. I tried them and they work fine. The
instructions are followed by warnings that illustrate why RAID is also spelled
“PITA.”

 

Volumes are just block devices. You should assume that you can do anything you
like that you’d normally be able to do with block devices in the distro on the
AMI you’re running. You could make, I dunno, logical volumes, for example.  

 

(After you’ve had your fill of RAID, don’t stop this instance just yet. As long
as it’s running, you can try out getting metadata from it, in the next
section.)

 

Good reasons for the other two -- object-oriented programming in the shell or
bicycling to Alaska? No, I can’t think of any either.

EC2 Instance Metadata

 

Ever cat out /proc/cpuinfo or /proc/meminfo to get information about your
machine? You can understand why you’d want the same sort of thing for the AWS
metadata about your instance.

 

For example, you’d like to be able to get the ami-id of the box you’re on with
something like this:

 

cat /proc/ami-id

 

No such luck.

 

You can get the data, but you have to get it from a pretend URL instead of a
pretend file. The analogue to /proc is http://169.254.169.254; the analogue of
cat, wget.  For example, to get the ami-id, you say

 

wget -O /dev/stdout -q http://169.254.169.254/latest/meta-data/ami-id

 

The analogy with /proc will have made clear that you can only get this info
from the machine you’re trying to get information about: the contents vary from
machine to machine.

 

The Barr book will tell you more.

User-supplied metadata

 

You can supply your very own metadata at launch time. Start the Console’s
launch wizard. The second “Instance details” screen will have a big box
labelled “User Data” where you can put it in.

The data are available at http://169.254.169.254/latest/user-data.

 

The Barr book illustrates this, and also shows how to get the data with a PHP
script. One warning: the example uses a string of key-value pairs separated by
commas. On my Kindle, these look like periods. They’re commas. Honest.

Dynamic Diagramming

The final example in this big chapter, ec2_diagram.php, is very fancy. It
generates a GIF diagram of your EC2 resources, stuffs it into an S3 bucket for
your viewing pleasure, and hands you a URL where you can find it. You’ll need
to have an S3 bucket around for the task, so if you’ve gotten rid of all yours,
you can create a new one with create_bucket.php from Chapter 4.

 

You’ll also need to tell it the name of your bucket on the command line:

 

ec2_diagram.php <your-bucket-name-here>

 

The code in the book assumes a bucket name; I’ve just modified it in the same
way I modified the scripts in Chapter 4.

 

This work is licensed under the Creative Commons Attribution 3.0 Unported
License. To view a copy of this license, visit http://creativecommons.org/
licenses/by/3.0/ or send a letter to Creative Commons, 171 Second Street, Suite
300, San Francisco, California, 94105, USA.

 

Edit this page (if you have permission)–Published by Google Docs–Report Abuse
–Updated automatically every 5 minutes
